{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# data-handling dependencies\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn scaling & splitting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading and Preprocessing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load the PlentyOfFish image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "strImgPath = '../ek_scrape/img/doctordata/MyCity'\n",
    "lstImg = []\n",
    "for root, dirs, lstFile in os.walk(strImgPath):\n",
    "    for strFile in lstFile:\n",
    "        if strFile[-4:] == '.png':\n",
    "            img = pyplot.imread(strImgPath + '/' + strFile, format='jpg')\n",
    "            arrImg = image.img_to_array(img)\n",
    "            lstImg.append(arrImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 110, 110, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrX = np.array(lstImg)\n",
    "arrX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfY = pd.read_csv('doctordata_MyCity.csv')\n",
    "arrY = np.array(dfY['interested'])\n",
    "arrY.shape\n",
    "arrY.ndim\n",
    "arrY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Train and Test datasets\n",
    "arrTrainX, arrTestX, arrTrainY, arrTestY = train_test_split(arrX, arrY, random_state=17) # , stratify=arrY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For Logistic Regression, we want to flatten our data into rows of 1D image arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (169, 36300)\n",
      "Testing Shape: (57, 36300)\n"
     ]
    }
   ],
   "source": [
    "# transform the 110x110x3 pics to a flat 1D array\n",
    "fltDimCount = arrTrainX.shape[1] * arrTrainX.shape[2] * arrTrainX.shape[3]\n",
    "arrTrainX = arrTrainX.reshape(arrTrainX.shape[0], fltDimCount)\n",
    "arrTestX = arrTestX.reshape(arrTestX.shape[0], fltDimCount)\n",
    "print(\"Training Shape:\", arrTrainX.shape)\n",
    "print(\"Testing Shape:\", arrTestX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# normalize training data\n",
    "scaler = MinMaxScaler().fit(arrTrainX)\n",
    "arrTrainX = scaler.transform(arrTrainX)\n",
    "arrTestX = scaler.transform(arrTestX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our first step is to create an empty sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create first hidden layer, 100 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Add the first layer where the input dimensions are the 784 pixel values\n",
    "# We can also choose our activation function. `relu` is a common\n",
    "model.add(Dense(100, activation='relu', input_dim=arrTrainX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create second hidden layer, 100 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Add a second hidden layer\n",
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our final output layer uses a `softmax` activation function for logistic regression.\n",
    "\n",
    "We also need to specify the number of output classes. In this case, the number of digits that we wish to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Add our final output layer where the number of nodes \n",
    "# corresponds to the number of y labels\n",
    "model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# uses categorical hinge for \"interested\" (1) and \"not interested\" (0)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_hinge', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finally, we train our model using our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Training consists of updating our weights using our optimizer and loss function. In this example, we choose 10 iterations (loops) of training that are called epochs.\n",
    "\n",
    "We also choose to shuffle our training data and increase the detail printed out during each training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.7041 - acc: 0.1479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c25b62fc88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit (train) the model\n",
    "model.fit(\n",
    "    arrTrainX,\n",
    "    arrTrainY,\n",
    "    epochs=10,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('pofmodel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We use our testing data to validate our model. This is how we determine the validity of our model (i.e. the ability to predict new and previously unseen data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 1.6842 - acc: 0.1579\n",
      "Loss: 1.6842105409555268, Accuracy: 0.15789473056793213\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the training data \n",
    "fltLoss, fltAccuracy = model.evaluate(arrTestX, arrTestY, verbose=2)\n",
    "print(f'Loss: {fltLoss}, Accuracy: {fltAccuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "We can use our trained model to make predictions using `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = np.expand_dims(arrTrainX[0], axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction\n",
    "model.predict(test).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = np.expand_dims(arrTrainX[2], axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c25c6f51d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(lstImg[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction. The resulting class should match the digit\n",
    "print(f\"One-Hot-Encoded Prediction: {model.predict(test).round()}\")\n",
    "print(f\"Predicted class: {model.predict_classes(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../Images/test8.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "image_size = (28, 28)\n",
    "im = image.load_img(filepath, target_size=image_size, color_mode=\"grayscale\")\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array \n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "image = img_to_array(im)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the image pixels by 255 (or use a scaler from sklearn here)\n",
    "image /= 255\n",
    "\n",
    "# Flatten into a 1x28*28 array \n",
    "img = image.flatten().reshape(-1, 28*28)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the pixel values to match the original data\n",
    "img = 1 - img\n",
    "plt.imshow(img.reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.predict_classes(img)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
