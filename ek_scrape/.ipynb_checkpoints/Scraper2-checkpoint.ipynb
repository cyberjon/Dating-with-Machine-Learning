{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from config import strUsername\n",
    "from config import strPassword\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "import requests\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function: initialize browser\n",
    "def init_browser():\n",
    "    dictPath = {\"executable_path\": \"chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **dictPath, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PopulateList(soup, strTag, strClass, fReplaceDoubleReturns):\n",
    "    lstRaw = soup.find_all(strTag, class_= strClass)\n",
    "    lstOut = []\n",
    "    if fReplaceDoubleReturns:\n",
    "        for strItem in lstRaw:\n",
    "            lstOut.append(strItem.text.replace(\"\\n\\n\", \"\").replace(\"\\n\", \" \"))\n",
    "    else\n",
    "        for strItem in lstRaw:\n",
    "            lstOut.append(strItem.text.replace(\"\\n\", \"\"))\n",
    "    return lstOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function: scrape profile\n",
    "def profileScrape(soup):\n",
    "    dictProfile = {}\n",
    "    \n",
    "    # get username and headline\n",
    "    strUser = soup.find(id = \"username\").text\n",
    "    strHeadline = soup.find(id = \"headline\").text\n",
    "\n",
    "    # add to dict\n",
    "    dictProfile[\"username\"] = strUser\n",
    "    dictProfile[\"headline\"] = strHeadline\n",
    "\n",
    "    # GET INFO FROM PROFILE TABLE 1\n",
    "    # part 1: find header names for first profile table\n",
    "    lstOutHead1 = PopulateList(soup, \"div\", class_= \"profileheadtitle\", False)\n",
    "    # part 2: find first half of content (left side of table)\n",
    "    lstOutAbout1 = PopulateList(soup, \"div\", class_= \"profileheadcontent\", True)\n",
    "    # part 3: find 2nd half of content (right side of table)\n",
    "    lstOutAbout2 = PopulateList(soup, \"div\", class_= \"profileheadcontent-narrow\", True)\n",
    "    # part 4: combine both content into one list\n",
    "    arrOutAbout = np.array([lstOutAbout1, lstOutAbout2])\n",
    "    lstOutAbout = np.ravel(arrOutAbout, order = \"F\")\n",
    "\n",
    "    # define dictionary to hold data in the first profile table. keys = header/title, value = content\n",
    "    dictProfile1 = dict(zip(lstOutHead1, lstOutAbout))\n",
    "\n",
    "    # add to dict\n",
    "    dictProfile[\"profile_info_1\"] = dictProfile1\n",
    "\n",
    "    # GET INFO FROM PROFILE TABLE 2\n",
    "    lstRawAbout3 = soup.find_all(\"span\", class_=\"headline\")\n",
    "    lstOutAbout3 = []\n",
    "    for i in range(9, 25):\n",
    "        lstOutAbout3.append(lstRawAbout3[i].text.replace(\"\\n\", \"\"))\n",
    "        \n",
    "    lstRawAbout4 = soup.find_all(\"table\")[1](\"span\", class_=\"txtGrey\")\n",
    "    lstOutAbout4 = []\n",
    "    for strItem in lstRawAbout4:\n",
    "        lstOutAbout4.append(strItem.text.replace(\"\\n\", \"\"))\n",
    "\n",
    "    dictProfile2 = dict(zip(lstOutAbout3, lstOutAbout4))\n",
    "\n",
    "    # add to dict\n",
    "    dictProfile[\"profile_info_2\"] = dictProfile2\n",
    "\n",
    "    # INTERESTS\n",
    "    try:\n",
    "        lstInterests = soup.find(id=\"interests\").text.split(\"\\n\")[1:4]\n",
    "    except: \n",
    "        lstInterests = \"None\"\n",
    "        print(\"No interests were entered\")\n",
    "    dictProfile[\"interests\"] = lstInterests\n",
    "\n",
    "    # ABOUT ME DESCRIPTION\n",
    "    lstDesc = soup.find(id=\"description\").text.replace(\"\\n\", \"\")\n",
    "    lstDescSplit = lstDesc.split(\" \")\n",
    "    dictProfile[\"about_me_text\"] = lstDesc\n",
    "    dictProfile[\"about_me_split\"] = lstDescSplit\n",
    "\n",
    "    return dictProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) initialize browser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = init_browser()\n",
    "\n",
    "# define initial login page and the pages we want to visit after logging in ##\n",
    "pof_login_url = 'https://login.pof.com' #define login url (need to add username and password) \n",
    "pof_online = 'https://www.pof.com/basicsearch.aspx' # currently online url - page 1\n",
    "pof_base = 'https://www.pof.com/' # base url\n",
    "pof_nextpage = 'everyoneonline.aspx?page_id=' # base next_page\n",
    "\n",
    "# define initial vars\n",
    "count = 0\n",
    "profile_urls = []\n",
    "page = 2\n",
    "\n",
    "# visit login page\n",
    "browser.visit(pof_login_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> 4) (go to Chrome browser and manually input login credentials now)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) grab profile URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the \"who's online\" page\n",
    "browser.visit(pof_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 profile urls found\n",
      "2 profile urls found\n",
      "3 profile urls found\n",
      "4 profile urls found\n",
      "5 profile urls found\n",
      "6 profile urls found\n",
      "7 profile urls found\n",
      "8 profile urls found\n",
      "9 profile urls found\n",
      "10 profile urls found\n",
      "11 profile urls found\n",
      "12 profile urls found\n",
      "13 profile urls found\n",
      "14 profile urls found\n",
      "15 profile urls found\n"
     ]
    }
   ],
   "source": [
    "# grab profile URLs\n",
    "while len(profile_urls) < 10:\n",
    "\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    online_results = soup.find_all(\"div\", class_=\"results\")\n",
    "\n",
    "    for info in online_results:\n",
    "        profile_page = pof_base + info.find('a')['href']\n",
    "\n",
    "        if profile_page not in profile_urls:\n",
    "            profile_urls.append(profile_page)\n",
    "            print(f\"{count + 1} profile urls found\")\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    page += 1\n",
    "    try:\n",
    "        new_page = pof_base + soup.find(id = 'basicsearch_nextpage')['href']\n",
    "        browser.visit(new_page)\n",
    "    except:\n",
    "        print(\"last page\")\n",
    "        browser.visit(pof_base + 'abandon.aspx')\n",
    "        browser.visit(pof_login_url)\n",
    "        browser.fill(\"username\", \"serruhb\") \n",
    "        browser.fill(\"password\", \"password\")\n",
    "        browser.click_link_by_id(\"logincontrol_submitbutton\") # click submit to enter credentials\n",
    "        browser.visit(pof_online) # everyone who is online - page 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.pof.com/viewprofile.aspx?profile_id=236320807',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=120071900',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=127208365',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=117003873',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=256533738',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=214703599',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=259252328',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=150555529',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=111266984',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=254840748',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=220712278',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=217814998',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=66020257',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=107522039',\n",
       " 'https://www.pof.com/viewprofile.aspx?profile_id=258366067']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No interests were entered\n",
      "profile 1 of 15 scraped\n",
      "No interests were entered\n",
      "profile 2 of 15 scraped\n",
      "profile 3 of 15 scraped\n",
      "No interests were entered\n",
      "profile 4 of 15 scraped\n",
      "profile 5 of 15 scraped\n",
      "profile 6 of 15 scraped\n",
      "No interests were entered\n",
      "profile 7 of 15 scraped\n",
      "profile 8 of 15 scraped\n",
      "No interests were entered\n",
      "profile 9 of 15 scraped\n",
      "profile 10 of 15 scraped\n",
      "No interests were entered\n",
      "profile 11 of 15 scraped\n",
      "No interests were entered\n",
      "profile 12 of 15 scraped\n",
      "profile 13 of 15 scraped\n",
      "No interests were entered\n",
      "profile 14 of 15 scraped\n",
      "No interests were entered\n",
      "profile 15 of 15 scraped\n"
     ]
    }
   ],
   "source": [
    "# loop through and visit each profile page and scrape\n",
    "datamate_profiles = {}\n",
    "profile_num = 0\n",
    "\n",
    "for url in profile_urls:\n",
    "    browser.visit(url)\n",
    "\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    username = 'match_' + str(profile_num)\n",
    "    datamate_profiles[username] = profileScrape(soup)\n",
    "    print(f\"profile {profile_num + 1} of {count} scraped\")\n",
    "    profile_num += 1\n",
    "\n",
    "f = open(\"pof_output.txt\",\"w\")\n",
    "f.write(str(datamate_profiles))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamate_profiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
